# -*- coding: utf-8 -*-
"""1920-MS5114 PythonAssignmentTeam21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NKobw__I0sG3lOXBGOuTJ9Rc7TVnaSQy
"""
#Team21 
#1920-MS5114 ADVANCED PROGRAMMING FOR BUSINESS ANALYTICS(GROUP ASSIGNMENT)
#Saket Kumar - 19230134
#Anurag Gautam - 19230446
#RAJATH MELVAREGE RAMASWAMY - 19230221
#KAPIL ARORA - 19230069

#Import all the packages 
#pandas & numpy to read the csv and form array and perform operations
#matplotlib to plot the graphs and for visualisation
#sklearn is used to import the different models for the Machine learning

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import seaborn as sns
from shapely.geometry import Point, Polygon
import descartes
import random
import math
import time
import requests
from bs4 import BeautifulSoup

from sklearn.model_selection import RandomizedSearchCV,train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error,mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
import datetime
import operator

plt.style.use('seaborn')
# %matplotlib inline

#Reading the csv files using the pandas
confirmed_cases = pd.read_csv('/content/drive/My Drive/time_series_covid19_confirmed_global.csv')
death_cases = pd.read_csv('/content/drive/My Drive/time_series_covid19_deaths_global.csv')
recovered_cases = pd.read_csv('/content/drive/My Drive/time_series_covid19_recovered_global.csv')

from google.colab import drive
drive.mount('/content/drive')

#fecthing all the column values to extract the relevant data for visualisation and predcition
columnvalues = confirmed_cases.keys()
columnvalues

#extracting the useful data of total number of confirmed,death and recovery cases
data_confirmed = confirmed_cases.loc[:,columnvalues[4]:columnvalues[-1]]
data_death = death_cases.loc[:,columnvalues[4]:columnvalues[-1]]
data_recovered = recovered_cases.loc[:,columnvalues[4]:columnvalues[-1]]
print(data_death)

#Empty lists created to store the values of overall world cases and deaths and recovery between dates
dates = data_confirmed.keys() #gives the dates that is from 22nd Feb to 31st March
total_confirmed_cases = []
total_death_cases = []
total_recovery_cases = []

#running the for loop to find the total confirmed,deaths and recoverey cases in the world between the dates fetched
for i in dates:
  sum_total_confirmed = data_confirmed[i].sum() #gets the sum of all the confirmed cases between dates and stores in given variable
  sum_total_death = data_death[i].sum()
  sum_total_recovered = data_recovered[i].sum()
  total_confirmed_cases.append(sum_total_confirmed) #appends the value in the list
  total_death_cases.append(sum_total_death)
  total_recovery_cases.append(sum_total_recovered)

#Printing the value to check the numbers in the world
print("The total number of confirmed cases in world is",sum_total_confirmed)
print("The total number of death cases in world is",sum_total_death)
print("The total number of recovered cases in world is",sum_total_recovered)

#Converting all the dates and the cases in form of a numpy array and reshape is used to give a new shape to an array without changing its data
no_of_days = np.array([i for i in range(len(dates))]).reshape(-1,1)
confirmedworld_cases = np.array(total_confirmed_cases).reshape(-1,1)
total_Deaths = np.array(total_death_cases).reshape(-1,1)
total_Recovery = np.array(total_recovery_cases).reshape(-1,1)

confirmedworld_cases

#creating the prediction for next 10 days by adding it to the dates
predicted_days = 10
forecast_data = np.array([i for i in range(len(dates)+predicted_days)]).reshape(-1,1)
updated_dates = forecast_data[:-10]

#providing the details of the start date and covert it to a string using the strptime
starting_date = '1/22/2020'
start_date = datetime.datetime.strptime(starting_date,'%m/%d/%Y')
forecast_data_dates = []
for i in range(len(forecast_data)):
  forecast_data_dates.append((start_date+datetime.timedelta(days=i)).strftime('%m/%d/%Y'))

latest_confirmed_cases = confirmed_cases[dates[-1]]
latest_deaths_cases = death_cases[dates[-1]]
latest_recovered_cases = recovered_cases[dates[-1]]

unique_countries_list = list(confirmed_cases['Country/Region'].unique())

#calculate the total number of confimed cases by each country and remove the counrties with no cases found of covid-19
total_confirmed_cases_by_country=[]
no_cases_found = []
for i in unique_countries_list:
  cases = latest_confirmed_cases[confirmed_cases['Country/Region']==i].sum()
  if cases > 0:
    total_confirmed_cases_by_country.append(cases)
  else:
    no_cases_found.append(i)

for i in no_cases_found:
  unique_countries_list.remove(i) #remove the countries with no cases from the list

unique_countries_list = [k for k, v in sorted (zip(unique_countries_list,total_confirmed_cases_by_country),key= operator.itemgetter(1),reverse=True)]
for i in range(len(unique_countries_list)):
  total_confirmed_cases_by_country[i]= latest_confirmed_cases[confirmed_cases['Country/Region']==unique_countries_list[i]].sum()

#printing the values to check the confirmed cases by country based on the rank order and most confirmed cases
print('Condfimed cases by Countries/Regions:')
for i in range(len(unique_countries_list)):
  print(f'{unique_countries_list[i]}:{total_confirmed_cases_by_country[i]} cases')

#finding the list of provinces from the data with confirmed cases
unique_provinces_list = list(confirmed_cases['Province/State'].unique())

#finding the number of confirmed cases per province, state or city
province_confirmed_cases = []
no_cases = []
for i in unique_provinces_list:
  cases = latest_confirmed_cases[confirmed_cases['Province/State']==i].sum()
  if cases > 0:
    province_confirmed_cases.append(cases)
  else:
    no_cases.append(i)

#removing the areas with no cases
for i in no_cases:
  unique_provinces_list.remove(i)

#removing the NAN from the code
nan_indices = []

for i in range(len(unique_provinces_list)):
  if type(unique_provinces_list[i])== float:
    nan_indices.append(i)

unique_provinces_list = list(unique_provinces_list)
province_confirmed_cases = list(province_confirmed_cases)

for i in nan_indices:
  unique_provinces_list.pop(i)
  province_confirmed_cases.pop(i)

plt.figure(figsize=(20, 12))
plt.plot(updated_dates, confirmedworld_cases,linestyle='dashed')
plt.plot(updated_dates, total_Deaths)
plt.plot(updated_dates, total_Recovery,linestyle='dashdot')
plt.title('Number of Covid-19 Cases Over Time', size=30)
plt.xlabel('Days Since 1/22/2020', size=30)
plt.ylabel('Number of Cases', size=30)
plt.legend(['Confirmed Cases', 'Death Cases', 'Recovered Cases'], prop={'size': 20})
plt.xticks(size=20)
plt.yticks(size=20)
plt.show()

#Visualization of the total confirmed cases accross the countries

plt.figure(figsize=(40,42))
plt.barh(unique_countries_list[:40],total_confirmed_cases_by_country[:40])
plt.title("Number of confirmed cases in Countries",size=50)
plt.xlabel('Number of confirmed covid 19 cases',size=30)
plt.xticks(size=20)
plt.yticks(size=20)
plt.show()

#Country wise visualisation of the confirmed cases in the world
Ireland_confirmed = latest_confirmed_cases[confirmed_cases['Country/Region']=='Ireland'].sum()
UnitedStates_confirmed = latest_confirmed_cases[confirmed_cases['Country/Region']=='US'].sum()
Italy_confirmed = latest_confirmed_cases[confirmed_cases['Country/Region']=='Italy'].sum()
India_confirmed = latest_confirmed_cases[confirmed_cases['Country/Region']=='India'].sum()
China_confirmed = latest_confirmed_cases[confirmed_cases['Country/Region']=='China'].sum()
Spain_confirmed = latest_confirmed_cases[confirmed_cases['Country/Region']=='Spain'].sum()


plt.figure(figsize=(16,9))
plt.barh('Ireland',Ireland_confirmed)
plt.barh('United States',UnitedStates_confirmed)
plt.barh('Italy',Italy_confirmed)
plt.barh('India',India_confirmed)
plt.barh('China',China_confirmed)
plt.barh('Spain',Spain_confirmed)
plt.title('Number of confirmed Covid-19 Cases',size=30)
plt.show()

#top 10 countries with top cases of covid19

visual_unique_countries = []
visual_confirmed_Cases = []
others = np.sum(total_confirmed_cases_by_country[10:])
for i in range(len(total_confirmed_cases_by_country[:10])):
  visual_unique_countries.append(unique_countries_list[i])
  visual_confirmed_Cases.append(total_confirmed_cases_by_country[i])


visual_confirmed_Cases.append(others)
visual_unique_countries.append('Rest of world')

#visualise using bar grapgh top 10 countries

plt.figure(figsize=(32,18))
plt.barh(visual_unique_countries,visual_confirmed_Cases)
plt.title('Number of Covid-19 Confirmed cases in Countries/Regions',size=30)
plt.show()

#Visual of top 10 countries using pie charts

c= random.choices(list(mcolors.CSS4_COLORS.values()),k= len(unique_countries_list))
plt.figure(figsize=(20,20))
plt.title('Covid 19 Confirmed cases per country')
plt.pie(visual_confirmed_Cases,colors=c)
plt.legend(visual_unique_countries,loc='best')
plt.show()

#function to calculate the daily increase and decrease in the confirmed cases 
def daily_pattern(data):
    d = [] 
    for i in range(len(data)):
        if i == 0:
            d.append(data[0])
        else:
            d.append(data[i]-data[i-1])
    return d

world_daily_increase = daily_pattern(total_confirmed_cases)

#plotting the Daily Increase and Decrease cases in World
plt.figure(figsize=(20, 12))
plt.plot(updated_dates, world_daily_increase,linestyle='dashed')
plt.title('Daily Increase and Decrease cases in World', size=30)
plt.xlabel('Days Since 1/22/2020', size=30)
plt.ylabel('Number of Cases', size=30)
plt.legend(['Confirmed pattern'], prop={'size': 20})
plt.xticks(size=20)
plt.yticks(size=20)
plt.show()

#Predciting the next 10 days for the growth in the number of confirmed cases in world based on past data
#The models used will be SVM(Support Vector Machine),TransformedTargetRegressor

#Dividing the data into training and test sets using train_test_split()
X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(no_of_days, confirmedworld_cases, test_size=0.05, shuffle=False)

#Building the prediction using the SVM model
c = [0.01, 0.1, 1]
gamma = [0.01,0.1,1]
epsilon = [0.01,0.1,1]
shrinking = [True,False]

svm_grid ={'C':c,'gamma':gamma,'epsilon':epsilon,'shrinking':shrinking}

svm = SVR(kernel='poly')
svm_search = RandomizedSearchCV(svm,svm_grid,scoring='neg_mean_squared_error',cv=3,return_train_score=True,n_jobs=-1,n_iter=40,verbose=1)
svm_search.fit(X_train_confirmed,y_train_confirmed)

#searching and finding the best parameters out of all provided above
svm_search.best_params_

#gets the best estimate and the forecast using predict()
svm_confirmed = svm_search.best_estimator_
svm_pred = svm_confirmed.predict(forecast_data)

#Number of Coronavirus Cases Over Time
plt.figure(figsize=(20, 12))
plt.plot(updated_dates, confirmedworld_cases)
plt.title('Number of Coronavirus Cases Over Time', size=30)
plt.xlabel('No of Days after 1/22/2020', size=30)
plt.ylabel('Number of Cases', size=30)
plt.xticks(size=20)
plt.yticks(size=20)
plt.show()

plt.figure(figsize=(16, 9))
plt.plot(updated_dates, confirmedworld_cases)
plt.plot(forecast_data, svm_pred, linestyle='dashed', color='purple')
plt.title('No. of Coronavirus Cases Over Time', size=30)
plt.xlabel('Days Since 1/22/2020', size=30)
plt.ylabel('No. of Cases', size=30)
plt.legend(['Confirmed Cases', 'Predictions using SVM'], prop={'size': 20})
plt.xticks(size=20)
plt.yticks(size=20)
plt.show()

#dividing the data into a ratio of 60 and 40
X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(no_of_days[60:], confirmedworld_cases[60:], test_size=0.05, shuffle=False)

#Building the prediction using the SVM model
c = [0.01, 0.1, 1]
gamma = [0.01,0.1,1]
epsilon = [0.01,0.1,1]
shrinking = [True,False]

svm_grid ={'C':c,'gamma':gamma,'epsilon':epsilon,'shrinking':shrinking}

svm = SVR(kernel='poly')
svm_search = RandomizedSearchCV(svm,svm_grid,scoring='neg_mean_squared_error',cv=3,return_train_score=True,n_jobs=-1,n_iter=40,verbose=1)
svm_search.fit(X_train_confirmed,y_train_confirmed)

#creating the svm model for predciting values from 12th march onwards
svm_confirmed = SVR(kernel='poly')
svm_confirmed.fit(X_train_confirmed, y_train_confirmed)
svm_pred = svm_confirmed.predict(forecast_data)

plt.figure(figsize=(20, 12))
plt.plot(updated_dates[50:], confirmedworld_cases[50:])
plt.plot(forecast_data[59:], svm_pred[59:], linestyle='dashed', color='purple')
plt.title('No of Coronavirus Cases in World-wide Over Time', size=30)
plt.xlabel('Days Since 3/12/2020', size=30)
plt.ylabel('No of Cases', size=30)
plt.legend(['Confirmed Cases', 'SVM predictions'])
plt.xticks(size=15)
plt.show()

#printing the predicted cases for next 10 days
print('SVM future prediction')
set(zip(forecast_data_dates[-10:],svm_pred[-10:]))

X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(no_of_days, confirmedworld_cases, test_size=0.05, shuffle=False)

#using the Transformed target regressor to find the future confirmed cases worldwide
from sklearn.linear_model import RidgeCV
from sklearn.compose import TransformedTargetRegressor

regr = TransformedTargetRegressor(regressor=RidgeCV(),func=np.log1p,inverse_func=np.expm1)
regr.fit(X_train_confirmed, y_train_confirmed)
regr_pred = regr.predict(forecast_data)

print('Accuracy of TransformedTargetRegressor on test set: {:.2f}'.format(regr.score(X_test_confirmed, y_test_confirmed)))

plt.figure(figsize=(20, 12))
plt.plot(updated_dates[:], confirmedworld_cases[:])
plt.plot(forecast_data, regr_pred, linestyle='dotted', color='red')
plt.title('No of Coronavirus Cases Worldwide Over Time', size=30)
plt.xlabel('Days Since 1/22/2020', size=30)
plt.ylabel('No of Cases', size=30)
plt.legend(['Confirmed Cases', ' logarithmic  and an exponential transformed Ridge Regression  predictions'])
plt.xticks(size=15)
plt.show()

print('Transformed Target Regressor future pred')
print(regr_pred[-10:])

#hitting worldometer website to fetch current live data
url = "https://www.worldometers.info/coronavirus/"
#using requests package to get requests
r = requests.get(url)
data= r.text
soup = BeautifulSoup(data,'html.parser')

# checking the title of the webpage and removing its tag
print(soup.title.text)
data_capturedFromSite = soup.find_all('div',id='maincounter-wrap')
#html data from site
print(data_capturedFromSite)
# printing the required data from all html page 
for i in data_capturedFromSite:
  print(i.text)

print('Corona data based on countries')
#extracting worldometer data
table_body= soup.find('tbody')
table_rows= table_body.find_all('tr')
# creating 5 empty lists 
countries =[]
cases = []
todays_freshcases=[]
deaths = []
recovered = []

for tr in table_rows:
  # fetching td details for all countries
  td = tr.find_all('td')
  # appending the lists
  countries.append(td[0].text)
  cases.append(td[1].text)
  todays_freshcases.append(td[2].text)
  deaths.append(td[3].text)
  recovered.append(td[5].text)

#making a dataframe with headers as stated
headers=['Countries','Total Cases','Fresh Cases','Deaths','Recovered']
#assigning all values to df named corona_df
corona_df= pd.DataFrame(list(zip(countries, cases, todays_freshcases,deaths,recovered)),columns=headers)
print(corona_df)

#from all cases we are getting the values of world cases
totalCases=corona_df['Total Cases'].iloc[0]
totalDeaths = corona_df['Deaths'].iloc[0]
totalRecovered = corona_df['Recovered'].iloc[0]
#printing the total cases, recovered and deaths worldwide
print("Total confirmed cases in the world as of today:",totalCases)
print("Total deaths in the world as of today:",totalDeaths)
print("Total recovered patients in the world as of today:",totalRecovered)

height = [totalCases,totalRecovered,totalDeaths]
bars = ('Deaths', 'Recovered', 'Total Cases')
y_pos = np.arange(len(bars))
plt.figure(figsize=(15,10))
plt.bar(y_pos,height[::-1], color=(0.2, 0.4, 0.6))
plt.title("Bar Graph showing deaths, recovered and total cases thoroughout the world")
plt.xticks(y_pos, bars)
plt.show()